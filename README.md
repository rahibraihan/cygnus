Higher-Order Methods and Richardson Extrapolation for Numerical Differentiation : 
This repository contains our group assignment on numerical differentiation techniques, exploring how different methods compare in terms of accuracy and convergence. We implemented and compared various numerical differentiation methods to approximate derivatives of smooth functions. The main goal was to understand how higher-order methods improve accuracy and how Richardson extrapolation can reduce errors.

URL: https://github.com/rahibraihan/cygnus.git

What We Did

Implemented second-order central difference method (O(h²))

Formula: f'(x) ≈ [f(x+h) - f(x-h)] / (2h)
Error: O(h²)


Implemented fourth-order central difference method (O(h⁴))

Formula: f'(x) ≈ [-f(x+2h) + 8f(x+h) - 8f(x-h) + f(x-2h)] / (12h)
Error: O(h⁴)


Applied Richardson extrapolation to improve accuracy

Formula: D_R = [4D(h/2) - D(h)] / 3


Analyzed errors, stability, and convergence behavior
Generated plots and tables to visualize results


Test Function
We used f(x) = sin(x) as our test function with the exact derivative f'(x) = cos(x). The derivative was evaluated at x = 1.0 using different step sizes (h = 0.1, 0.05, 0.025, ...).

System Requirements

Compiler: C++
Visualization: Gnuplot


Repository Structure

CYGNUS Report.pdf → Group assignment report (PDF)
CYGNUS Report.zip → Zipped version of the report and related files
Cygnus_cpp → C++ source code for numerical differentiation
Cygnus.exe → Compiled executable
LICENSE → License information
README.md → Project documentation
error_data.txt → Numerical error data generated by the program
error_plot.png → Error convergence plot
plot.gp → Gnuplot script used to generate the plot


Key Insights

Higher-order methods significantly reduce truncation error.
Fourth-order schemes outperform second-order schemes.
Richardson extrapolation improves accuracy.
Choosing an appropriate step size is critical for numerical stability.


